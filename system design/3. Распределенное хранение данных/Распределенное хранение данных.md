
# Репликация

- Репликация - создания клона базы данных для быстрого подхвата функций поврежденной системы
- Бэкап - это резервное копирование содержимого диска с целью последующего восстановления 

## Надежность - поддержка резервной базы данных на случай потери основной
## Масштабирование чтение - снижает нагрузки на чтение за счет переноса части запросов на реплики

## Виды ролей в репликации

### Master - slave 

![[Master - slave.png]]
- пишем в мастер
- читаем из слейвов или из мастера
- в случае падения мастера получаем downtime на запись

### Failover - процесс замены master( в случае downtime) на slave 
![[Failover.png]]
### Hot standby -  помечаем следующую реплику, что она станет master
![[hot stanby.png]]

### Split brain - алгоритм "поколений"

### Master-master 
![[Master-master.png]]
- Конфликты
- Пишем в разные мастера
- Читаем из слейвов или мастеров
- В случае падения мастера нет никаокго downtime на запись

#### Конфликты
- LWW(last win write)  - выбирается последния добавленная запись
- Ранг реплик - ранжирование мастеров взависимости от ранга
- Решение конфликтов на клиенте - клиент сам будет определять какие данные ему выбрать
- CRDT(Conflict-replicated data type) - 


### Master-less

![[master-less.png]]
- пишем в определенные ноды
- читаем с определенные нод
- W + R > n - строгая согласованность
- W + R <= N - не гарантируется строгая согласованность
- R = 1 и W = N -  система оптимизированная для быстрого чтения
- W = 1 и R = N - система оптимизированна для быстрой записи

### Типы репликаций 

#### **Strong Consistency - любая операция чтения из любого узла базы данных вернет последнюю операцию записи**

#### Синхронная репликация 
1. Запись транзакции в журнал
2. Применение транзакции в движке
3. Отправка данных на все реплики
4. Получение подтверждения от всех реплик
5. Возвращение подтверждения клиенту
![[sync-replication.png]]


#### **Eventual Consistency - В отсутствии изменений данных, через какой-то промежуток времени после последнего обновление( в конечном счете) все запросы будут возвращать последнее обновленное значение**

#### Асинхронная 
1. Запись транзакции в журнал
2. Применение транзакции в движке
3. Возвращение подтверждении клиенту
4. отправка данных на реплики

![[async-replication.png]]

#### Replication lag - задержка в репликации между узлами на случай если пользователь записал и идет сразу читать эти данные

![[replication-lag.png]]


### Формат передачи данных что и как будем перекидывать

#### Источник передачи данных 
- push -  мастер рассылает данные репликам (pgSQL)
- pull - реплики стягивают данные сами (MySQL)

### Statement base Replication
1. Передаются запросы(но не сущности)
2. Каждый запрос считается на каждой ноде(проблема с random(), unix_timestamp)

### Row based Replication
1. Передаются измененные строчки в бинарном виде
2. Передаются сущности, даже если изменили одно поле ( есть возможность настроить, какие колонки будут отправляться(full/minimal))

### Mixed - база данных переключается между SBR в SBR или из RBR в SBR, в зависимости от той ли иной ситуации

### Логическая репликация 
- Работает с кортежами(SBR, RBR)
- Не знает, как они хранятся на диске

### Физическая репликация
- Работает со страцицами(передает всю информацию)
- slave = master(байт в байт) (репликация состояние жестких дисков)


### CAP теорема - утверждение о том, что в любой реализации распределенных вычислений возможно обеспечить не более двух и трех следующих свойств(согласованность, доступность, устойчивость к разделению)

### Консистентность - данные на всех нодах одинаковы

![[CAP-consistency.png]]

#### Доступность - если запрос пришел на живую ноду, запрос будет получен за конечное время

![[CAP-availability.png]]

#### Устойчивость к разделению - продолжаем работать не смотря на отсутствие связи 

![[CAP - partition-tolerantnost.png]]


#### Обрубаем связь
1. Невозможно обеспечить постоянную связь P AC - система
2. Разрешаем из нод читать, но запрещаем писать CP - система
3. Разрешаем писать и разрешаем читать(если связи нет, но мы не можем писать ) - AP Система

## Партиционирование - разделение больших таблиц на много маленьких(в идеале секции находятся на одном и том же инстансе базы данных)

### Вертикальное партиционирование

![[vertical-partition.png]]

### Горизонтальное партиционирование

![[horizontal-partition.png]]

## Шардирование - разделение большой таблицы на независимые  сегменты, каждый из которых управляется отдельным инстансом базы данных
![[sharding.png]]


### Способы шардирования 

#### Range based
![[range-based-sharding.png]]
- Проблемы с переполнениями одного из шарда
#### Key based
![[key-based-sharding.png]]

- Подсчет хэша и вычисления шарда с помощью модуля по количество шардов
### directory based

![[directory-based-sharding.png]]

- Шардирование относительно места использованиям ( к примеру регионы, города, и т.д.)

### Routing 

#### Клиентский

![[client-routing.png]]
- + Нет лишних узлов
- - дополнительная логика на клиенте
- - сложности с обновление хостов

#### Proxy
![[proxy-routing.png]]

- + приложение не знает о шардинге
- - дополнительный сетевой узел
- - потеря функциональности(если сложный запрос по разным шардам - невозможно)
- - единичная точка отказа

#### Coordinator(Умный proxy)
![[coordinator-routing.png]]
- + кэширование
- + приложение не знает о шардинге
- - дополнительный сетевой узел
- - инфраструктурная сложность
- - единичная точка отказа
- - нагрузка

### Перебалансировка шардов
#### Virtual buckets
![[virtual-buckets-rebalance.png]]

#### Перебалансировка
1. Только чтение
2. Все данные неизменяемые (пишем в tgt(куда переливаем), читаем из src(из которого переливаем) и tgt)
3. Логическая репликация с src на tgt, после синхронизации переключаемся на tgt
4. Смешанный подход

#### Resharding

#### Hashing
![[hashing-resharding.png]]
- если добавляем шарды с системой shard то будут проблемы с ключами

#### Consistent Hashing
![[consistent-hashing-resharding.png]]
Значение как-то хэшится и переносится в следующий по пути шард
- неравномерная нагрузка
- если упал шард из-за нагрузки то упадет каскадно следующие шарды, для решения проблемы используем виртуалные шарды

![[virtualShard-consistent-hashing-resharding.png]]

#### Randezvous Hashing

![[randezvous-hashing.png]]


### Kafka cluster
![[kafka-cluster.png]]

### Capture Data Change(Debezium)

![[capture-data-change.png]]
